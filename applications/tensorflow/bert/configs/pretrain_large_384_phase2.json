{
    "task": "pretraining",
    "attention_probs_dropout_prob": 0.0,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 1024,
    "initializer_range": 0.02,
    "max_position_embeddings": 512,
    "num_attention_heads": 16,
    "num_hidden_layers": 24,
    "hidden_layers_per_stage": [2,2,2,2,2,2,2,2,2,2,2,2],
    "type_vocab_size": 2,
    "vocab_size": 30528,
    "seq_length": 384,
    "batch_size": 6,
    "batches_per_step": 1,
    "steps": 2142,
    "optimiser_epsilon": 1e-6,
    "max_predictions_per_seq": 58,
    "lr_schedule": "polynomial_decay",
    "warmup": 277,
    "base_learning_rate": 0.002828,
    "loss_scaling": 128,
    "optimiser": "lamb",
    "pipeline_depth": 2730,
    "parallell_io_threads": 16,
    "pipeline_schedule": "Grouped",
    "replicas": 1,
    "precision": "16",
    "seed": 1234,
    "logits_matmul_serialization_factor": 6,
    "scheduler": "Clustering",
    "steps_per_ckpts": 100,
    "steps_per_logs": 1,
    "weight_decay": 0.01,
    "disable_graph_outlining": false,
    "restoring":false,
    "no_logs": false,
    "do_validation":false,
    "do_train":true,
    "ipu_function": true,
    "available_memory_proportion":0.11,
    "partials_type": "half",
    "embeddings_placement": "two_ipus",
    "reduction_type": "mean",
    "duplication_factor": 5,
    "train_file": "/path/to/tokenised/wikipedia/*",
      
    "checkpoint_path": "./checkpoint/phase2",
    "checkpoint_model":"./checkpoint/phase2/model"
  }
  
